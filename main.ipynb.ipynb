{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installing dependencies:**"
      ],
      "metadata": {
        "id": "f1pjyOPzON8T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu_haU9VKkkU",
        "outputId": "793624cf-6d13-4cce-eed4-68ef4aaaff48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m553.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=3184306 sha256=7ca14f01ed9ca02e40429615d1595b6339635ae5c2be41f983f34b5a6084216d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lm75u3_8/wheels/a5/73/ee/670fbd0cee8f6f0b21d10987cb042291e662e26e1a07026462\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.2.0\n",
            "Cloning into 'kenlm'...\n",
            "remote: Enumerating objects: 14165, done.\u001b[K\n",
            "remote: Counting objects: 100% (478/478), done.\u001b[K\n",
            "remote: Compressing objects: 100% (331/331), done.\u001b[K\n",
            "remote: Total 14165 (delta 163), reused 409 (delta 133), pack-reused 13687\u001b[K\n",
            "Receiving objects: 100% (14165/14165), 5.91 MiB | 17.85 MiB/s, done.\n",
            "Resolving deltas: 100% (8043/8043), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/kpu/kenlm/archive/master.zip\n",
        "!git clone https://github.com/kpu/kenlm.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing usage:**"
      ],
      "metadata": {
        "id": "yVGcVRJKO8rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kenlm\n",
        "model = kenlm.Model('/content/kenlm/lm/test.arpa')\n",
        "print(model.score('this is a sentence .', bos = True, eos = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGah9tNzPAUx",
        "outputId": "61ba22c0-1527-42d9-b960-cc18445bc926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-49.579345703125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compiling:**"
      ],
      "metadata": {
        "id": "lPdplasAQMW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kenlm/build\n",
        "!cmake /content/kenlm -B /content/kenlm/build\n",
        "%cd /content/kenlm/build\n",
        "!make -j 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l18XdBYpPeJD",
        "outputId": "ce44b8c9-38e2-4434-b3ad-76e13ea0b92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
            "-- Configuring done (0.1s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "/content/kenlm/build\n",
            "[ 38%] Built target kenlm_util\n",
            "[ 41%] Built target probing_hash_table_benchmark\n",
            "[ 46%] Built target kenlm_filter\n",
            "[ 71%] Built target kenlm\n",
            "[ 73%] Built target query\n",
            "[ 76%] Built target build_binary\n",
            "[ 78%] Built target fragment\n",
            "[ 81%] Built target kenlm_benchmark\n",
            "[ 83%] Built target filter\n",
            "[ 86%] Built target phrase_table_vocab\n",
            "[ 95%] Built target kenlm_builder\n",
            "[ 97%] Built target lmplz\n",
            "[100%] Built target count_ngrams\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yzk84pjQJWj",
        "outputId": "bb64c2ad-0aa6-4396-9568-244b7ca028db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "kenlm  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing (optional):**"
      ],
      "metadata": {
        "id": "C5sv1Yy9b1ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the training dataset. It can be a text file or a compressed file (bzip2). In case you would like to compress a text file and then pass it to the training function, here's how you do it:\n",
        "\n",
        "\n",
        "```\n",
        "bzip2 <input-file.txt>\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "US2n89UDQxwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model:**\n",
        "\n",
        "Parameters can be provided, which include:\n",
        "\n",
        "\n",
        "1.   Order\n",
        "2.   Threshold\n",
        "3.   Input file path\n",
        "\n"
      ],
      "metadata": {
        "id": "v5_71Chxb9Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "def train_language_model(lmplz_path, order, threshold, input_file):\n",
        "    \"\"\"\n",
        "    Train a language model using lmplz tool.\n",
        "\n",
        "    Args:\n",
        "    - lmplz_path (str): Path to the lmplz executable.\n",
        "    - order (int): Order of the n-gram model.\n",
        "    - threshold (int): Threshold count for pruning low-frequency n-grams.\n",
        "    - input_file (str): Path to the input text file.\n",
        "\n",
        "    Returns:\n",
        "    - output_file (str): Path to the ARPA format output file.\n",
        "    \"\"\"\n",
        "    # Generate output file name based on input file name\n",
        "    output_file = os.path.splitext(input_file)[0] + \"_model.arpa\"\n",
        "\n",
        "    command = [lmplz_path, '-o', str(order), '-T', str(threshold), '<', input_file, '>', output_file]\n",
        "    command_str = ' '.join(command)\n",
        "\n",
        "    # Run the command and capture the output and error\n",
        "    process = subprocess.Popen(command_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "\n",
        "    # Decode and print the output\n",
        "    if stdout:\n",
        "        print(\"Output:\\n\", stdout.decode())\n",
        "    if stderr:\n",
        "        print(\"Error:\\n\", stderr.decode())\n",
        "\n",
        "    return output_file\n",
        "\n",
        "# Example usage:\n",
        "lmplz_path = \"/content/kenlm/build/bin/lmplz\"\n",
        "# Set n-gram order\n",
        "order = 3\n",
        "#  Specifies a threshold count for pruning low-frequency n-grams from the model.\n",
        "# N-grams occurring fewer times than the specified threshold will be pruned.\n",
        "threshold = 5\n",
        "# Enter path of input file:\n",
        "input_file = \"/content/Shah.txt\"\n",
        "\n",
        "output_file = train_language_model(lmplz_path, order, threshold, input_file)\n",
        "print(\"Output file:\", output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyRo5k5EROb7",
        "outputId": "0f4fa635-5c83-46e3-e233-7abfd70baeef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error:\n",
            " === 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/Shah.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 143982 types 15207\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:182484 2:3786914560 3:7100464640\n",
            "Statistics:\n",
            "1 15207 D1=0.725766 D2=0.990402 D3+=1.43806\n",
            "2 61176 D1=0.825489 D2=1.24297 D3+=1.42936\n",
            "3 86798 D1=0.888037 D2=1.08122 D3+=0.915233\n",
            "Memory estimate for binary LM:\n",
            "type      kB\n",
            "probing 3345 assuming -p 1.5\n",
            "probing 3763 assuming -r models -p 1.5\n",
            "trie    1535 without quantization\n",
            "trie     943 assuming -q 8 -b 8 quantization \n",
            "trie    1463 assuming -a 22 array pointer compression\n",
            "trie     872 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:182484 2:978816 3:1735960\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:182484 2:978816 3:1735960\n",
            "=== 5/5 Writing ARPA model ===\n",
            "Name:lmplz\tVmPeak:10780364 kB\tVmRSS:8968 kB\tRSSMax:2477608 kB\tuser:0.286305\tsys:1.28289\tCPU:1.56921\treal:1.54644\n",
            "\n",
            "Output file: /content/Shah_model.arpa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing generated model:**"
      ],
      "metadata": {
        "id": "dlHT0IiacnHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import os\n",
        "import kenlm\n",
        "\n",
        "# LM = os.path.join(os.path.dirname(__file__), '..', 'lm', 'test.arpa')\n",
        "#model = kenlm.LanguageModel(LM)\n",
        "\n",
        "model = kenlm.Model(output_file)\n",
        "print('{0}-gram model'.format(model.order))\n",
        "\n",
        "sentence = 'language modeling is fun .'\n",
        "print(sentence)\n",
        "print(model.score(sentence))\n",
        "\n",
        "# Check that total full score = direct score\n",
        "def score(s):\n",
        "    return sum(prob for prob, _, _ in model.full_scores(s))\n",
        "\n",
        "assert (abs(score(sentence) - model.score(sentence)) < 1e-3)\n",
        "\n",
        "# Show scores and n-gram matches\n",
        "words = ['<s>'] + sentence.split() + ['</s>']\n",
        "for i, (prob, length, oov) in enumerate(model.full_scores(sentence)):\n",
        "    print('{0} {1}: {2}'.format(prob, length, ' '.join(words[i+2-length:i+2])))\n",
        "    if oov:\n",
        "        print('\\t\"{0}\" is an OOV'.format(words[i+1]))\n",
        "\n",
        "# Find out-of-vocabulary words\n",
        "for w in words:\n",
        "    if not w in model:\n",
        "        print('\"{0}\" is an OOV'.format(w))\n",
        "\n",
        "#Stateful query\n",
        "state = kenlm.State()\n",
        "state2 = kenlm.State()\n",
        "#Use <s> as context.  If you don't want <s>, use model.NullContextWrite(state).\n",
        "model.BeginSentenceWrite(state)\n",
        "accum = 0.0\n",
        "accum += model.BaseScore(state, \"a\", state2)\n",
        "accum += model.BaseScore(state2, \"sentence\", state)\n",
        "#score defaults to bos = True and eos = True.  Here we'll check without the end\n",
        "#of sentence marker.\n",
        "assert (abs(accum - model.score(\"a sentence\", eos = False)) < 1e-3)\n",
        "accum += model.BaseScore(state, \"</s>\", state2)\n",
        "assert (abs(accum - model.score(\"a sentence\")) < 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXJq1WnYY4jU",
        "outputId": "2a317538-894a-4b03-9375-f91cc267e888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3-gram model\n",
            "language modeling is fun .\n",
            "-19.566287994384766\n",
            "-5.348799705505371 1: language\n",
            "-4.914098262786865 1: modeling\n",
            "\t\"modeling\" is an OOV\n",
            "-2.1186938285827637 1: is\n",
            "-3.5201516151428223 2: is fun\n",
            "-3.2292842864990234 1: .\n",
            "-0.43526071310043335 2: . </s>\n",
            "\"modeling\" is an OOV\n"
          ]
        }
      ]
    }
  ]
}